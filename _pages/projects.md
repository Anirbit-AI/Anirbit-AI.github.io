---
layout: page
title: Focus Projects
permalink: /projects/
description: Ideas We Are Excited About!
nav: true
nav_order: 5
---

<style>
  .responsive-container {
    display: flex;
    align-items: center;
    gap: 20px;
  }

  @media (max-width: 768px) {
    .responsive-container {
      flex-direction: column;
      align-items: flex-start; /* optional, aligns text to the left */
    }
  }
</style>

<!-- _pages/publications.md -->
<div class="responsive-container">
<img src="/assets/img/finalized-2.png" alt="Publications Banner" style="width:175px; height:175px;"/>
 <div>
     <p style="margin-top: 5px;"> 
     Partial differential equations (PDEs) are the ubiquitous way to model various natural dynamics such as fluid flow or evolution of the quantum state of a molecule. Various sciences have a critical dependency on having fast and reliable numerical solvers for PDEs. However classical methods of simulating PDEs can be very expensive for very many instances of practical relevance. Hence large parts of the industry are trying to implement methods from Scientific-ML for these tasks. 
      </p> 
     <p style="margin-top: 5px;">
   In our group we seek to explore the foundational challenges of Scientific-ML via the lens of mathematics - and push the frontiers of both. We aim to prove (a) architecture requirements for neural PDE solving and (b) performance guarantees and (c) uncertainty quantification for (stochastic) first-order algorithms that can be deployed to train neural operators and nets for solving target PDEs and dynamical systems, in arbitrary dimensions.
     </p>
  </div>
</div>

<div>
 <p>  </p>
</div>
<div>
 <p>  </p>
</div>
<!-- _pages/publications.md -->
<div style="display:flex;align-items;center; gap: 20px;">
<img src="/assets/img/finalized.png" alt="Publications Banner" style="width:175px; height:175px;"/>
 <div>
  <h3> Mathematical Foundations of Scientific-ML</h3>
     <p style="margin-top: 5px;">
       <br>
     We gave the first-ever proof of,
       <br>
     &nbsp &nbsp • model size requirements for operator training,
       <br>
   [Size Lowerbounds for Deep Operator Networks (TMLR 2024)](https://openreview.net/pdf?id=RwmWODTNFE)
       <br>
      &nbsp &nbsp • sample size requirements for operator learning,
       <br>
   [Towards Size-Independent Generalization Bounds for Deep Operator Nets (TMLR 2024)](https://openreview.net/pdf?id=21kO0u6LN0)   
     </p>
  </div>
</div>
<div>
 <p>  </p>
</div>

<!-- _pages/publications.md -->
<div style="display:flex;align-items;center; gap: 20px;">
<img src="/assets/img/finalized.png" alt="Publications Banner" style="width:175px; height:175px;"/>
 <div>
   <h3>  Provable Neural Training Algorithms </h3>
     <p style="margin-top: 5px;"> </p>
  </div>
</div>

