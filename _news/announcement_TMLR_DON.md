---
layout: post
date: 2024-02-1 00:00:00-0400
inline: true
related_posts: false
---

<b>TMLR!</b>

[Size Lowerbounds for Deep Operator Networks](https://openreview.net/pdf?id=RwmWODTNFE)

This work is with [Amartya Roy](https://www.linkedin.com/in/amartyaroy7/?originalSubdomain=in) @ Bosch, India. As far as we know, this is among the rare few proofs of any kind of architectural constraint for training performance that has ever been derived for any neural architecture. And this is almost entirely data independent -- hence, a "universal lower bound" and in particular, the lower bounds we derive do not depend on the Partial Differential Equation being targetted to be solved by a DeepONet. Also, this analysis leads to certain "scaling law" conjectures for DeepONets as we touch upon in the short experimental section.

