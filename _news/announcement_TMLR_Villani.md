---
layout: post
date: 2024-02-25 00:00:00-0400
inline: true
related_posts: false
---

<b>TMLR!</b>

[Global Convergence of SGD For Logistic Loss on Two Layer Neural Nets](https://openreview.net/pdf?id=9TqAUYB6tC)

Here we give a first-of-its-kind proof of SGD convergence on finitely large neural nets - for logistic loss in the binary classification setting.  This continues our investigation that neural loss functions can be "Villani functions" and that uncovering this almost magical mathematical property of neural loss functions can help prove convergence to global minima of gradient-based algorithms for it. This is work with [Pulkit Gopalani](https://pulkitgopalani.github.io/) (PhD student at UMichigan) and Samyak Jha (undergrad at IIT-Bombay).
