---
layout: post
date: 2025-06-19 00:00:00-0000
inline: true
related_posts: false
---

<b>TMLR!</b>

[Regularized Gradient Clipping Provably Trains Wide and Deep Neural Networks](https://openreview.net/pdf?id=ABT1XQLbOx)

This is our 4th paper at the TMLR journal! <b> This gives the first ever example of adaptive gradient algorithms  which provably do deep-learning and competitively so. </b> 

In this work we invent a particualar class of step-length schedules which continuous interpolate between gradient descent and gradient clipping based on a parameter delta. We show that these intermediates (a) provably minimize squared losses on standard deep-nets at large widths parametric in the interpolating parameter delta and (b) also competitively train modern transformer based architectures. 
